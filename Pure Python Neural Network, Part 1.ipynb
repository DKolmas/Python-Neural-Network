{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note is the first one out of a few next notes desribing implementation and analysis of pure python neural network. The purpose to do this exercise was to grasp a hands on experience with pute Python implementation details as I am moving now to Tensor Flow (TF). TF is a great tool. However, it is good to understand basic concepts first. \n",
    "\n",
    "At the beggining I planned to prepare just one note but over time it became clear it would be way to much content to make it digestible even for very stuborn and motivated person.\n",
    "\n",
    "In this note I will describe:\n",
    " * my motivation for doing that exercise (you might find yourself in a similar position, perheaps)\n",
    " * architecture of neural network implemented in python\n",
    " * data samples used for learning and testing\n",
    " * two different approaches used in implementation of backrpoagation algorithm\n",
    " * three methods of weights initiaizations used in analysis\n",
    " \n",
    "In the next note I will focus on performance analysis of trained network. I will provide many figures and complement it with results discussion. Finnaly I will formulate my conclussions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While attending Udacity online course in 2017, **[Deep Learninig Foundation](https://eu.udacity.com/course/deep-learning-nanodegree-foundation--nd101)**, I could see a greate possibilities using Deep Learning (via TensorFlow) in different tasks: image recognition, automatic translator engine, ... These task involves complex network architectures: Convolutional Neural Networks (CNN), Recuent Neural Networks (RNN) (and some flavors of it), and my favorit General Adversial Netowkrs (GANs). \n",
    "\n",
    "This course was great. It showed me how to build quickly powerfool tools using Tensor Flow to handle very complex tasks. It was very motivating course and acutally it encoured me to do more. I completed the course with TensorFlow hands on experience and good overview of what deep learing can do for me.\n",
    "\n",
    "Also in my professional life I use Reinforced Learning where Neural Netowork is part of the whole framework. \n",
    "\n",
    "All of the above is more about applications of deep networks. At some point I decided to hold on for a \"moment\" and spent time to uderstand better theoretical aspects. And that is why I have created the note on pure python neural network.\n",
    "\n",
    "(Possibly remove following acapit)\n",
    "\n",
    "So far I can see already benefits of investing my time in this work. I can better understand NN related content I read therefore spending less time on digesting them (for eample nice explanation on [softmax in TensorFlow](https://stackoverflow.com/questions/34240703/whats-the-difference-between-softmax-and-softmax-cross-entropy-with-logits)). I can also further expand my knowledge based on my up to date uxperience. Shortly I plan to focus on understanding a concept and application of **Autoencoders** and **Actor-Critic Algorithms** among the others.\n",
    "\n",
    "So far I have also created other notes on:\n",
    " * [Foundation of backpropagation algorithm](https://dkolmas.github.io/blog/2017/11/09/Foundation-of-backpropagation)\n",
    " * [Softmax and cross-entropy functions in backpropagation algorithm](https://dkolmas.github.io/blog/2017/12/11/Softmax-function-and-cross-entropy-in-backpropagation-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of Neural Netowork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network (NN) architecture is very simple in presented use case. It contains input layer, one hidden layer and output layer:\n",
    "* there are 2 inputs in input layer\n",
    "* there are 100 nodes in hidden layer\n",
    "* there are 3 outputs in output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of inputs (2) and outputs (3) is dictated by a type of data I have used. Details on  data type will be discussed in **Data Samples** section. 100 nodes in hidden layer is arbitrary choosen. Figure below presents the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"Simple_NN_model_03.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer (L0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two inputs in the input layer (L0). The dimension of the input is determined by the data used for training and evaluation. More details on training data can be found in section about Data Samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden layer (L1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Activation function for nodes in the hidden layer (L1) is ReLu (Rectified Linear Unit). This is simply function which can be desribed with the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)=\\max(0,x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $x$ greater than $0$, the output of the hidden node takes the value of its input. Othrewise the output takes $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer (L2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput layer (L2) is a Softmax layer. Softmax function accepts a vector of size $C$ and returns at its output vector of the same size $C$ (see the figure below). Value of each element of the input vector $h$ is a value which is a weigthed sum of ouputs from layer L1. Each element of the input vector $h$ is called a logit. Input vector $h$ is transformed it into an output vector $\\hat y$. Each element of the ouput vector $\\hat y$ has a value between 0 and 1 and all the elements sums up to 1. Therefore value of each element of the output vector $\\hat y$ can be interpreted as probability.\n",
    "\n",
    "There are three ouputs and it is dictaded by the type of data used in the study. Each element of the ouput value in data samples represents a class an input value belongs to. Therefore each element of the output of the NN is interpreted as the probability a given input belongs to a class represented be the element of the ouput vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Softmax_in_out.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Neural Network used in this study case is very simple. I decided to have a look into examples already discussed in many sources (for example [CS231n Stanord lecture on Convolutional NN](http://cs231n.github.io/) or great [Neural Networks and Deep Learning free online book](http://neuralnetworksanddeeplearning.com/index.html)). While learning new staff it is good to have a good reference point. So not only architecture of NN is similar to examples I looked into, but also data samples type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below visualise data samples used in training and performance evaluation of NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Data_samples_01.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
